{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28934169-a38b-4a22-98d2-69ce7106e4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrices for W_q: \n",
      " tf.Tensor(\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\n",
      "\n",
      " Q: \n",
      " tf.Tensor(\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32)\n",
      "\n",
      "Attention Weights:\n",
      " [[0.42231882 0.15536243 0.42231882]\n",
      " [0.15536243 0.42231882 0.42231882]\n",
      " [0.21194157 0.21194157 0.57611686]]\n",
      "\n",
      "Output:\n",
      " [[0.84463763 0.57768124 0.84463763 0.57768124]\n",
      " [0.57768124 0.84463763 0.57768124 0.84463763]\n",
      " [0.7880584  0.7880584  0.7880584  0.7880584 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sample word embeddings (3 words, embedding dim = 4)\n",
    "word_embeddings = tf.constant([[1, 0, 1, 0],    # \"The\"\n",
    "                               [0, 1, 0, 1],    # \"Cat\"\n",
    "                               [1, 1, 1, 1]],   # \"Sat\"\n",
    "                              dtype=tf.float32)                # create a constant tensor\n",
    "\n",
    "# Weight matrices to compute Q, K, V (for simplicity: identity)\n",
    "W_q = tf.eye(4)\n",
    "W_k = tf.eye(4)\n",
    "W_v = tf.eye(4)\n",
    "print(\"Weight matrices for W_q: \\n\", W_q)\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q = word_embeddings @ W_q      # Multiply the word_embeddings matrix with the W_q matrix to get the Query matrix (Q).\n",
    "K = word_embeddings @ W_k\n",
    "V = word_embeddings @ W_v\n",
    "\n",
    "print(\"\\n Q: \\n\", Q)\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "d_k = tf.cast(tf.shape(K)[-1], tf.float32)           # d_k = 4         \n",
    "scores = Q @ tf.transpose(K) / tf.math.sqrt(d_k)     # Gives the attention score matrix (shape: [3, 3]) representing how much each word should attend to the others.\n",
    "\n",
    "# Apply softmax to get attention weights\n",
    "attention_weights = tf.nn.softmax(scores, axis=-1)   # converts raw scores into probabilities (attention weights) across each row.\n",
    "\n",
    "# Multiply attention weights by V to get output\n",
    "output = attention_weights @ V\n",
    "\n",
    "print(\"\\nAttention Weights:\\n\", attention_weights.numpy())\n",
    "print(\"\\nOutput:\\n\", output.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5242a-979b-4661-b64e-b2f7c3b8ee76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b732516-baf3-437e-8769-05a3fd115f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca170e21-b43b-421c-80fc-024be0970f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "994866e7-fe0e-41ce-8b5b-1afb9fa51d40",
   "metadata": {},
   "source": [
    "self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43f74467-4639-4027-8850-d4386d0c21b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention Weights:\n",
      " [[0.19777581 0.40111209 0.40111209]\n",
      " [0.40111209 0.19777581 0.40111209]\n",
      " [0.24825508 0.24825508 0.50348984]]\n",
      "\n",
      "Output:\n",
      " [[2.20333628 5.00556046]\n",
      " [2.         4.80222419]\n",
      " [2.25523477 5.26221445]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dummy embeddings for 3 tokens (e.g., \"I love NLP\")\n",
    "X = np.array([\n",
    "    [1, 0],   # Token 1\n",
    "    [0, 1],   # Token 2\n",
    "    [1, 1]    # Token 3\n",
    "])  # Shape: (3, 2)\n",
    "\n",
    "\n",
    "# Weight matrices (random example values for learning)\n",
    "Wq = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1]\n",
    "])  # Shape: (2, 2)\n",
    "\n",
    "Wk = np.array([\n",
    "    [0, 1],\n",
    "    [1, 0]\n",
    "])  # Shape: (2, 2)\n",
    "\n",
    "Wv = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "])  # Shape: (2, 2)\n",
    "\n",
    "\n",
    "Q = X @ Wq.T  # Shape: (3, 2)\n",
    "K = X @ Wk.T  # Shape: (3, 2)\n",
    "V = X @ Wv.T  # Shape: (3, 2)\n",
    "\n",
    "scores = Q @ K.T\n",
    "\n",
    "dk = Q.shape[-1]  # typically dim of key\n",
    "scaled_scores = scores / np.sqrt(dk)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # subtract max for numerical stability\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)\n",
    "\n",
    "output = attention_weights @ V\n",
    "\n",
    "\n",
    "print(\"\\nAttention Weights:\\n\", attention_weights)\n",
    "print(\"\\nOutput:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e068a-42ae-405a-b40e-ada1d51790bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
