{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db30060-286a-4243-b298-f250bf45eae7",
   "metadata": {},
   "source": [
    "### 1. Text Vectorization Layer in TensorFlow\n",
    "\n",
    "#### What It Is\n",
    "The TextVectorization layer in TensorFlow:\n",
    "Converts raw strings (sentences) into sequences of integers.\n",
    "\n",
    "Handles:\n",
    " * Lowercasing\n",
    " * Removing punctuation\n",
    " * Tokenizing into words or characters\n",
    " * Creating vocabulary\n",
    " * Mapping each token to an integer\n",
    "\n",
    "It’s super helpful when you want to build your own vocabulary from raw text rather than relying on datasets like IMDb that already return tokenized inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2395fa2c-0c77-45c0-8d96-14317dbd60d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " ['', '[UNK]', np.str_('this'), np.str_('movie'), np.str_('is'), np.str_('a'), np.str_('what'), np.str_('good'), np.str_('film'), np.str_('fantastic'), np.str_('bad')]\n",
      "\n",
      "Vectorized Output:\n",
      " [[ 2  4  5  7  3  0]\n",
      " [ 2  3  4 10  0  0]\n",
      " [ 6  5  9  8  0  0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Example corpus\n",
    "texts = [\n",
    "    \"This is a good movie\",\n",
    "    \"This movie is bad\",\n",
    "    \"What a fantastic film!\"\n",
    "]\n",
    "\n",
    "# Create TextVectorization layer\n",
    "\"\"\"\n",
    "This layer will:\n",
    "    1. Build a vocabulary of the most common words (up to max_tokens). Means most common 1000 word's vocabulary.\n",
    "    2. Convert text to integers, where each word maps to a unique integer (based on frequency).\n",
    "    3. Pad or truncate sequences to exactly output_sequence_length tokens.\n",
    "\"\"\"\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=1000,     # vocab size\n",
    "    output_mode='int',   # convert to integer IDs\n",
    "    output_sequence_length=6  # fixed length sequences\n",
    ")\n",
    "\n",
    "# Build vocabulary\n",
    "\"\"\"\n",
    "The adapt() method analyzes the text corpus and builds the vocabulary. \n",
    "It learns which words occur and assigns each word a unique index based on frequency (most frequent = lower index).\n",
    "We can see in output as \"this\", \"movie\", \"is\", \"a\" words occured multiple times that's why they got lower index means they occured at the beginning of list.\n",
    "\"\"\"\n",
    "vectorizer.adapt(texts)\n",
    "\n",
    "# View the vocabulary\n",
    "\"\"\"\n",
    "This shows the vocabulary that the layer has learned. Each word is assigned a unique index. The first two indices are usually reserved:\n",
    "    \"[PAD]\" → index 0 (used for padding)\n",
    "    \"[UNK]\" → index 1 (used for unknown/out-of-vocab words)\n",
    "\"\"\"\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "print(\"Vocabulary:\\n\", vocab)\n",
    "\n",
    "# Transform the text\n",
    "\"\"\"\n",
    "This line transforms the original sentences into sequences of integers using the vocabulary.\n",
    "Each sentence is converted into a list of 6 integers (as specified by output_sequence_length).\n",
    "\"\"\"\n",
    "vectorized_text = vectorizer(texts)\n",
    "# print(\"vectorized_text: \", vectorized_text)\n",
    "print(\"\\nVectorized Output:\\n\", vectorized_text.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb8609-c68f-4cd7-8156-b017501a3650",
   "metadata": {},
   "source": [
    "Then a sentence like `This is a good movie` might get transformed to `[ 2  4  5  7  3  0]` where:\n",
    "\n",
    "* this:2 \n",
    "* is:4 \n",
    "* a:5 \n",
    "* good:7 \n",
    "* movie:3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1d38a-6165-43bd-b4dd-87d0b4f3f3aa",
   "metadata": {},
   "source": [
    "### How TextVectorization Connects with Embedding\n",
    "\n",
    "#### Overview of Flow:\n",
    "Raw Text → TextVectorization → Sequence of Token IDs → Embedding → Dense Representation for each token\n",
    "\n",
    "\n",
    "Step-by-Step Connection:\n",
    "1. TextVectorization Layer\n",
    "    * Takes raw strings and converts them into a fixed-length integer sequence (As above o/p).\n",
    "    * Each integer represents a word/token from the vocabulary it learned.\n",
    "\n",
    "2. Embedding Layer\n",
    "    * Takes the token IDs (integers from above) and looks up a dense vector (embedding) for each token.\n",
    "    * The output is a 2D or 3D tensor where each word is represented by a dense vector of fixed size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "279004bc-fba0-4a7c-9919-d6af1aab512f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf constant:  tf.Tensor([b'TensorFlow is powerful' b'I love deep learning'], shape=(2,), dtype=string)\n",
      "Embedded Output Shape: (2, 10, 16)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Step 1: TextVectorization Layer\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=10\n",
    ")\n",
    "\n",
    "# Sample data\n",
    "texts = [\"TensorFlow is powerful\", \"I love deep learning\"]\n",
    "vectorizer.adapt(texts)\n",
    "\n",
    "# Step 2: Embedding Layer\n",
    "\"\"\"\n",
    "vocab_size: The number of unique tokens including special tokens like [PAD] and [UNK].\n",
    "embedding_dim=16: Each word/token will be represented as a dense vector of 16 dimensions.\n",
    "\"\"\"\n",
    "vocab_size = len(vectorizer.get_vocabulary())  # must match!\n",
    "embedding_dim = 16\n",
    "\n",
    "# Step 3: Create model\n",
    "\"\"\"\n",
    "Below model includes:\n",
    "\n",
    "vectorizer: Converts input text to integer sequences.\n",
    "Embedding: Converts those integers into dense 16-dimensional vectors.\n",
    "mask_zero=True: Tells the model to ignore padding (0s) during training.\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    vectorizer,\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)\n",
    "])\n",
    "\n",
    "# Forward pass on example\n",
    "print(\"tf constant: \", tf.constant(texts))\n",
    "output = model(tf.constant(texts))\n",
    "# print(\"output: \", output)\n",
    "print(\"Embedded Output Shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0750d4c-5594-4e44-a6f4-eebbac837710",
   "metadata": {},
   "source": [
    "This pipeline:\n",
    "1. Converts raw text into integer sequences using a vocabulary.\n",
    "2. Maps those integers to dense 16D vectors using an Embedding layer.\n",
    "\n",
    "\n",
    "We are using `Sequential()` model because:\n",
    "1. First, the input text goes into the TextVectorization layer, which converts text to integers.\n",
    "2. Next, those integers go into the Embedding layer, which turns them into dense vectors.\n",
    "\n",
    "##### We're using Sequential to do all tasks in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b045b-8996-4324-8956-6637c146538b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
