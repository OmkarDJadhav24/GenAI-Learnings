{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8032749-f456-40bc-a1df-a819b14df44c",
   "metadata": {},
   "source": [
    "# Text Classification with Custom Data using TextVectorization, Embedding, and LSTM\n",
    "\n",
    "## Objective\n",
    "This notebook demonstrates an end-to-end NLP pipeline using TensorFlow for a small custom dataset. We preprocess raw text, vectorize it, embed it into dense vectors, and use an LSTM model to classify sentiments.\n",
    "\n",
    "## Key Concepts Covered\n",
    "\n",
    "- **TextVectorization**: A preprocessing layer that converts raw text into sequences of integers.\n",
    "- **Embedding Layer**: Maps each integer (representing a word) to a dense vector of fixed size.\n",
    "- **LSTM Layer**: A type of Recurrent Neural Network that captures long-term dependencies in text.\n",
    "- **Dense Layer**: A fully connected layer that outputs the final classification result.\n",
    "\n",
    "## Pipeline Steps\n",
    "\n",
    "1. **Input Raw Text & Labels**: We manually define a few positive and negative movie reviews with labels.\n",
    "2. **Text Vectorization**:\n",
    "   - Limits vocabulary size to 1000 most frequent tokens.\n",
    "   - Converts text to padded sequences of integers of length 20.\n",
    "3. **Build Model**:\n",
    "   - Input: Vectorized sequences\n",
    "   - Embedding Layer: Learns a 64-dimensional vector for each token\n",
    "   - LSTM Layer: Processes sequences to extract context\n",
    "   - Dense Output Layer: Produces binary sentiment classification (positive or negative)\n",
    "4. **Train & Evaluate**:\n",
    "   - The model is compiled with binary cross-entropy loss and trained over a few epochs.\n",
    "\n",
    "## Outcome\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How to handle custom text data in TensorFlow\n",
    "- How vectorization connects with the embedding layer\n",
    "- How to build a complete LSTM-based classifier without relying on built-in datasets like IMDb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a7e1e-f5e2-4262-bbc5-9c338b66d239",
   "metadata": {},
   "source": [
    "#### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8dbe83c-071d-49de-aa9d-d20b71b6beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9509d134-d012-48d4-84a4-439d8fb2c096",
   "metadata": {},
   "source": [
    "#### 2. Sample Text and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9238f3f7-388c-4ef8-892c-65defdac90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I love this movie, it's fantastic!\",\n",
    "    \"What a terrible film, I hated it.\",\n",
    "    \"Absolutely brilliant! Must watch.\",\n",
    "    \"Worst movie ever. Total waste of time.\",\n",
    "    \"An okay film, not too bad but not great.\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1]  # 1 = Positive, 0 = Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b523e3-2c04-4f1f-98cc-821e11bcbb80",
   "metadata": {},
   "source": [
    "#### 3. TextVectorization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76f06cb-f654-4293-ab26-3ccbaa72a683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 10:42:59.043478: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 1000\n",
    "max_sequence_length = 20\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This layer will:\n",
    "    1. Build a vocabulary of the most common words (up to max_tokens). Means most common 1000 word's vocabulary.\n",
    "    2. Convert text to integers, where each word maps to a unique integer (based on frequency).\n",
    "    3. Pad or truncate sequences to exactly output_sequence_length tokens. \n",
    "       Means if the length of the sequence list is less than `output_sequence_length(20 in our case)` then add 0's at the end(padding) \n",
    "       and if greater than `output_sequence_length` then remove values from sequence list to match the length of `output_sequence_length`. \n",
    "\"\"\"\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=max_vocab_size, output_mode='int', output_sequence_length=20)\n",
    "\n",
    "\"\"\"\n",
    "The adapt() method analyzes the text corpus and builds the vocabulary. \n",
    "It learns which words occur and assigns each word a unique index based on frequency (most frequent = lower index).\n",
    "\"\"\"\n",
    "vectorizer.adapt(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38e794-04b9-4427-b904-3ea500134c99",
   "metadata": {},
   "source": [
    "#### 4. Vectorize the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "240a0cf6-0922-4cdd-8248-b3780ccc392c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4 18 13  3 19 23  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 7 30 14  5  4 21 20  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [29 26 17  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 6  3 24 10  9 16 12  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [28 15  5  2 11 27 25  2 22  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This line transforms the original sentences into sequences of integers using the vocabulary.\n",
    "Each sentence is converted into a list of 20 integers (as specified by the output_sequence_length) and then that list is converted into numpy array.\n",
    "\"\"\"\n",
    "print(vectorizer(np.array([[s] for s in texts])).numpy())\n",
    "X = vectorizer(np.array([[s] for s in texts])).numpy()\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a04900-914e-4561-a7d4-098778ea3673",
   "metadata": {},
   "source": [
    "#### 5. Build the Model with Embedding + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97e61089-d8af-40b2-9f35-d11dbc90f945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omkarjadhav/miniconda3/envs/spacy-env/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([Embedding(input_dim=max_vocab_size, output_dim=64, input_length=max_sequence_length, mask_zero=True),\n",
    "                  LSTM(64),\n",
    "                  Dense(1, activation=\"sigmoid\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb2761b-a7da-4e01-b88b-b37694609575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7375 - loss: 0.6929\n",
      "Epoch 2/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6125 - loss: 0.6835\n",
      "Epoch 3/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.6776\n",
      "Epoch 4/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.6658\n",
      "Epoch 5/5\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.6514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7de8b4088080>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, epochs=5, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a0933-4530-4efa-b00f-27876525a23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb6b00-e38d-4254-8e1d-1e4d602a8b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
