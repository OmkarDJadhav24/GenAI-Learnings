{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2cde62-4772-42f8-a8f0-bb785c66abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c485849-5f75-4a98-aa2b-c6b44792f9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/omkarjadhav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595a139b-7171-42b4-b429-ce5184277831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example_string = \"\"\"Working on Text preprocessing. Natural Language Processing is amazing. Enjoy the learning.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da452e-5677-476b-babe-3d293f6a45bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5c8156e-d024-4458-963f-17892119fff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Working on Text preprocessing.',\n",
       " 'Natural Language Processing is amazing.',\n",
       " 'Enjoy the learning.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c1352b-e36d-4fd5-a416-dd404388b300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Working',\n",
       " 'on',\n",
       " 'Text',\n",
       " 'preprocessing',\n",
       " '.',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'amazing',\n",
       " '.',\n",
       " 'Enjoy',\n",
       " 'the',\n",
       " 'learning',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2bd49a-07bc-4d3e-9e33-d1428ed1b0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ad6fe-819e-4780-b43e-dbe0fdcc45b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ccced14-9dce-440c-8278-78e02db8663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/omkarjadhav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\"\"\"Let's tokenize example_string by word and store the resulting list in words_in_list\"\"\"\n",
    "words_in_list = word_tokenize(example_string)\n",
    "\n",
    "\"\"\"\n",
    "We have a list of the words in `words_in_list`, so the next step is to create a set of stop words to filter `words_in_list`. \n",
    "For this example, we’ll need to focus on stop words in `english`.\n",
    "\"\"\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\"\"\"Create an empty list, to hold all the words in words_in_list that aren’t stop words.\"\"\"\n",
    "filtered_list = [word for word in words_in_list if word.lower() not in stop_words]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd19398-4dbb-47ab-8aca-b7fa3b1f35b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Working', 'Text', 'preprocessing', '.', 'Natural', 'Language', 'Processing', 'amazing', '.', 'Enjoy', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253dcb26-bc23-4363-a97c-ba80b43484b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e60c98-c7ef-4703-8817-b916645ef461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'text', 'preprocess', '.', 'natur', 'languag', 'process', 'amaz', '.', 'enjoy', 'learn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_list]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664336e-60ca-4e1e-9fa3-f08e2f19a238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea58114-f497-4636-ae85-9acbbc8dd82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee28fc9e-cab0-4ad7-a834-9abdd28fbf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/omkarjadhav/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "string_for_lemmatizing = \"The friends of DeSoto love scarves.\"\n",
    "filtered_list = word_tokenize(string_for_lemmatizing)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_list]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1721b549-7809-476d-a11d-70587c82b83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worst'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"worst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbf0eb08-dfec-404d-8176-d0e8378ae2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"worst\", pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1decaa8d-6236-4ece-9fe4-68157e7a5981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /home/omkarjadhav/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "\n",
    "nltk.help.upenn_tagset('NN')  # Replace 'NN' with any tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1536a84-71b0-4954-b951-435ec7bf4551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96e283-96c1-4ac7-856c-80509d9e760b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f7b350a-8dcb-46a7-8aae-338ac9d35aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/omkarjadhav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/omkarjadhav/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Machine', 'NN'),\n",
       " ('learning', 'NN'),\n",
       " ('models', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('improving', 'VBG'),\n",
       " ('every', 'DT'),\n",
       " ('day', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = \"Machine learning models are improving every day.\"\n",
    "\n",
    "text_words = word_tokenize(text)\n",
    "tags = pos_tag(text_words)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba6ae0-4f64-4fb0-8883-535cda24343e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed06c24-b442-4a0b-8c34-042abb7adfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2a860-e937-4dad-b984-6b26447f22fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfa2b5fc-14d1-45d5-b3a3-31d0b63d09c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/omkarjadhav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/omkarjadhav/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumps', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "words = word_tokenize(sentence)\n",
    "tags = pos_tag(words)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7d390e8-3603-42dc-838c-bded91f510b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e19f1a01-a1a6-42df-a2f7-9cb4ba43a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree = cp.parse(tags)\n",
    "tree.draw()  # Opens a tree window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5985851f-a656-4ec5-a30c-b843f0e5e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<.*>+}         # Chunk everything\n",
    "      }<VB.*>{        # Chink (exclude) verbs\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree = cp.parse(tags)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92acbf64-ef26-4231-b380-b6f8d46abf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae36a2-1fd4-4d9b-a80c-f69311f065ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f34fb790-4d9a-4da5-961a-e686836756e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<.*>+}         # Chunk everything\n",
    "      }<VB.*>{        # Then exclude verbs\n",
    "\"\"\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree = cp.parse(tags)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8882171-8f48-40b1-acb6-c94959b0a4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c8fc72-e07e-4605-b6ca-cd6bba44b1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b248383b-0c7d-4eb6-83e0-0ac05ad58eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c41e6-f090-42b2-bc04-27b27b37b242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
