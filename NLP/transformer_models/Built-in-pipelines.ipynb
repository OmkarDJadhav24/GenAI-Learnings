{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9372d233-cbba-4a12-8376-05a1c0950428",
   "metadata": {},
   "source": [
    "This notebook uses `Hugging Face’s pipeline()` to access powerfull models like `gpt2`, `BERT`, `T5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04978241-44ad-4c8b-955a-7a0ca7859332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 13:30:00.682835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749715200.697839    8836 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749715200.702343    8836 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749715200.714047    8836 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749715200.714062    8836 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749715200.714063    8836 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749715200.714065    8836 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 13:30:00.718297: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d6d4c-23c6-4f09-b934-daca457f270c",
   "metadata": {},
   "source": [
    "#### Text Generation (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81589cf-36e8-4a79-9be0-1f6525b74c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942621d5-b0f4-4f1e-9946-6a3cba4b715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3611256-01aa-4f9c-9a9e-2f18e8f2f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45cf6259-6bad-4273-ad39-8ac73ce765c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"The future of AI is a question of how much of it is used in other areas, and whether it is being used as a means of improving the efficiency of AI.\\n\\nIn recent years, the number of AI jobs has grown to more than 20,000 a year from about 10,000. A lot of these jobs may be related to machines. The question is whether those jobs are being used for AI-related and related activities, or whether the jobs are being used for AI-related research.\\n\\nThese are the problems that I have identified as being at the heart of the problem. I don't think there are going to be big problems in the future.\\n\\nFor example, there will be big advances in the development of a computer. But there will be more people that are going to start working on AI-related problems. The problem is that the problem is not going to be solved by machines.\\n\\nThe question is if the problem is going to be solved by artificial intelligence. I'm not sure that will be the case.\\n\\nI think the problem is that AI will be involved in the future. I think we're going to see a lot of computer scientists and other professionals working on AI and computational problems.\\n\\nRobotics\\n\\nI think there will\"}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here are some Parameters used in below code:\n",
    "\n",
    "- prompt (str) : \"The future of AI is\" -  The initial input text for which the model will generate a continuation.\n",
    "- max_length (int) -                      The maximum number of tokens (including the input prompt) in the output text.\n",
    "- num_return_sequences (int) -            The number of distinct text sequences to return for the given prompt.\n",
    "                                          (if num_return_sequences=2 it will generate 2 sequences with 30 tokens for each)\n",
    "\n",
    "It will generate text upto max_length(including input prompt length) as one sequences.\n",
    "\"\"\"\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")   # Initializes a text generation pipeline using a specific pre-trained model.\n",
    "result = generator(\"The future of AI is\", max_length=30, num_return_sequences=1)\n",
    "# print(result[0][\"generated_text\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de029c15-d150-4b42-91ac-1a4bbec3bb01",
   "metadata": {},
   "source": [
    "#### 2. Sentiment Analysis (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eaea163-ebf1-452f-8252-54e1c9d66ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9971315860748291}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.999782383441925}]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline(\"sentiment-analysis\") # Initialize a pipeline for sentiment analysis using default pre-trained model.\n",
    "result = sentiment_analyzer(\"I love using Hugging Face transformers!\")\n",
    "print(result)\n",
    "\n",
    "result = sentiment_analyzer(\"I gets bored after 20 mins of study\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635bf9e-910b-452e-a954-72731ab46e3d",
   "metadata": {},
   "source": [
    "#### 3. Question Answering (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e47bc13b-ae18-452e-b376-e63e72a166bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.926306962966919, 'start': 60, 'end': 64, 'answer': '2017'}\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "result = qa_pipeline({\n",
    "    \"context\":\"Transformers are a deep learning architecture introduced in 2017.\",\n",
    "    \"question\":\"When were transformers introduced?\"\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c04bc-e496-44d2-8447-02577a7be097",
   "metadata": {},
   "source": [
    "#### 4. Summarization (T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6e2594b-f267-4fed-80f1-a579a18c78cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'the Transformer model, introduced in the paper \"Attention is all You Need\" has revolutionized natural language processing . the model is the foundational architecture behind many advanced models .'}]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "text = \"\"\"\n",
    "The Transformer model, introduced in the paper \"Attention is All You Need\", \n",
    "has revolutionized natural language processing by becoming the foundational architecture behind many advanced models, \n",
    "including BERT, GPT, and T5, enabling significant progress in various NLP tasks.\n",
    "\"\"\"\n",
    "\n",
    "result = summarizer(text, max_length=30, min_length=10, do_sample=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391dd14-3f80-4db3-b6e9-cebdc5fa5fb8",
   "metadata": {},
   "source": [
    "#### 5. Named Entity Recognition (NER)\n",
    "`grouped_entities=True`:\n",
    "* When set to True, it groups together tokens that belong to the same named entity.\n",
    "* For example, without grouping, \"Barack\" and \"Obama\" might be separate entities; with grouping, they'll be combined as one: \"Barack Obama\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cf4a6dd-8984-4c84-8601-a21db5b62c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9992892, 'word': 'Barack Obama', 'start': 0, 'end': 12}, {'entity_group': 'LOC', 'score': 0.9995566, 'word': 'Hawaii', 'start': 25, 'end': 31}, {'entity_group': 'LOC', 'score': 0.99947286, 'word': 'United States', 'start': 64, 'end': 77}]\n"
     ]
    }
   ],
   "source": [
    "# NER identifies entities like people, locations, dates, etc.\n",
    "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n",
    "result = ner_pipeline(\"Barack Obama was born in Hawaii and became the president of the United States.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758f364-4173-4450-9641-bd9f3851d4e7",
   "metadata": {},
   "source": [
    "#### 6. Translation (English → Hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cde385a-fe1f-494c-8403-d7fa64733795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77e86f8-d156-4d90-995d-2b45925ee11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'मुझे एआई और मशीन सीखने के बारे में सीखना अच्छा लगता है.'}]\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_hi\", model=\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "result = translator(\"I love learning about AI and machine learning.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d14229c-c04d-4ab3-8f34-4720096740d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
