{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a501ca10-3a7c-4a02-aac8-f4d1df03acdc",
   "metadata": {},
   "source": [
    "### ðŸ§© Components of a Hugging Face Pipeline\n",
    "A pipeline in Hugging Face is essentially made up of three core parts:\n",
    "\n",
    "**Tokenizer** â€“ Prepares input text for the model.\n",
    "\n",
    "**Model** â€“ Performs the task (e.g., classification, translation).\n",
    "\n",
    "**Pipeline Logic** â€“ Ties tokenizer + model together and applies task-specific behavior.\n",
    "\n",
    "Letâ€™s explore each one with a real example (`sentiment analysis`):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd22f0f-6966-43dd-9d8f-ecc1efe2f181",
   "metadata": {},
   "source": [
    "#### Step-by-step Breakdown\n",
    "1. Load Tokenizer: Tokenizers break text into tokens and convert them to IDs the model understands.\n",
    "\n",
    "AutoTokenizer automatically loads the correct tokenizer based on the model name.\n",
    "\n",
    "In our case the model is `distilbert-base-uncased-finetuned-sst-2-english`.\n",
    "\n",
    "`from_pretrained(...)` downloads the tokenizer configuration and vocabulary for this specific model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac30465-e085-43aa-9e11-a954c70668f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f750e9-a03d-47b1-b38e-372de395454b",
   "metadata": {},
   "source": [
    "2. Load Model: This is the trained neural network that understands the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a345b9c-717f-488e-be56-a088cced1f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\"\"\" This class helps you load a pre-trained model(in our case \"distilbert-base-uncased-finetuned-sst-2-english\") specifically designed for classifying sequences of text. \"\"\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26e32a-379d-4e54-8f51-a250241de51b",
   "metadata": {},
   "source": [
    "3. Build Pipeline Manually: You can combine tokenizer and model using the `pipeline` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154a88c4-869f-4fc5-9d17-4acacc814d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1f877e-c425-4a4d-8575-cd4c6bc00c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998455047607422}]\n"
     ]
    }
   ],
   "source": [
    "result = sentiment_pipeline(\"I love hugging face\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7667fc3-13e7-4fbd-9d4e-06d5e19f4042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3fb1b4-b6a5-4369-a376-bb50e4e6d761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cb8eab4-2e1c-4261-a0c3-6c6289c5ace8",
   "metadata": {},
   "source": [
    "## Tokenizer internals (how tokenization actually works)\n",
    "\n",
    "#### What is Tokenization?\n",
    "Tokenization is the process of converting text into pieces (called tokens) that can be converted into numbers and fed to a model.\n",
    "\n",
    "Input: \"Hugging Face is awesome!\"\n",
    "\n",
    "Tokens: [\"hugging\", \"face\", \"is\", \"awesome\", \"!\"]\n",
    "\n",
    "\n",
    "### Types of Tokenizers\n",
    "There are 4 main types of tokenizers youâ€™ll encounter in Hugging Face:\n",
    "\n",
    "| Type                         | Example         | Splits Words Into                         |\n",
    "| ---------------------------- | --------------- | ----------------------------------------- |\n",
    "| **Whitespace**               | basic tokenizer | \\[\"Hello\", \"world\"]                       |\n",
    "| **WordPiece**                | BERT            | \\[\"play\", \"##ing\"]                        |\n",
    "| **Byte Pair Encoding (BPE)** | RoBERTa, GPT    | \\[\"hug\", \"ging\", \"face\"]                  |\n",
    "| **Unigram**                  | XLNet           | \\[\"hugging\", \"face\", \"is\", \"aw\", \"esome\"] |\n",
    "\n",
    "\n",
    "These tokenizers help in:\n",
    "\n",
    "* Handling unknown words (unseenword â†’ un, ##seen, ##word)\n",
    "* Reducing vocabulary size\n",
    "* Improving model efficiency\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25d3e412-8279-4927-9ace-13cbb39893ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hugging', 'face', 'is', 'awesome', '!']\n",
      "Token IDs: [17662, 2227, 2003, 12476, 999]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Hugging Face is awesome!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5dcd2-91c6-4114-807c-99986f122fd9",
   "metadata": {},
   "source": [
    "Behind the scenes:\n",
    "\n",
    "* Text is lowercased (bert-base-uncased)\n",
    "* Punctuation is separated\n",
    "* Words like hugging are part of the vocabulary (so no subwords needed)\n",
    "\n",
    "\n",
    "\n",
    "#### What Happens Internally?\n",
    "Hereâ€™s the pipeline:\n",
    "\n",
    "1. Pre-tokenization: Clean, lowercase, and split by space/punctuation.\n",
    "2. Tokenization: Break tokens into subwords based on modelâ€™s vocabulary.\n",
    "3. Mapping to IDs: Use a vocabulary dictionary to convert tokens to integers.\n",
    "4. Padding/truncation: Add [PAD] if sentence is too short or truncate if too long.\n",
    "5. Special Tokens: Add [CLS] and [SEP] (used by BERT, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91dec3ea-2326-4ea1-aa80-6490e15432c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 17662,  2227,  2003, 12476,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Example: Full Tokenizer Output\n",
    "\n",
    "output = tokenizer(\"Hugging Face is awesome!\", return_tensors=\"pt\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf4082-da90-478b-af14-94761fe56866",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "* input_ids: The token IDs\n",
    "* attention_mask: Which tokens are real vs. padding\n",
    "* (optionally) token_type_ids: Used in tasks like QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d9058-785d-4486-9e1d-c48adcd79959",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "* Hugging Face tokenizers use subword tokenization for flexibility and compact vocabularies.\n",
    "* They convert raw text â†’ tokens â†’ IDs â†’ tensors.\n",
    "* You can inspect tokenization using .tokenize() and .convert_tokens_to_ids()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec26a36-34b3-46f4-806c-21379a3c9a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f260d45-ae62-42ba-9249-086d3b710d3d",
   "metadata": {},
   "source": [
    "Example Attention Mask and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d9e125c-8b7f-4511-a5a3-a3fcd08ed419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7592, 2088,  102,    0,    0,    0],\n",
      "        [ 101, 2129, 2024, 2017, 2651, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\"Hello world\", \"How are you today?\"], padding=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20cedb-1d63-401a-baa7-f3f1bb9b872e",
   "metadata": {},
   "source": [
    "The output contains:\n",
    "\n",
    "* input_ids: tokenized + padded sequences\n",
    "* attention_mask: 1s for real tokens, 0s for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf91e91-508e-4cfb-9561-d9cbe37b0e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
