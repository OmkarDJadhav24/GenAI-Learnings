{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d9f00-b2f3-49b4-a9b9-a70e162f6463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3f7e9b7-08a9-44fa-9c79-e676a2b2d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Step 1: Load word index mapping\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Step 2: Shift indices to account for special tokens\n",
    "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
    "\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# Step 3: Reverse index to get words from IDs\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "# Step 4: Load GloVe vectors\n",
    "embedding_index = {}\n",
    "embedding_dim = 100\n",
    "\n",
    "with open(\"../GloVe/glove.6B.100d.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vector\n",
    "\n",
    "# Step 5: Create embedding matrix for the words in the IMDb dataset\n",
    "vocab_size = 10000\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "# print(\"embedding_matrix: \", embedding_matrix)\n",
    "for i in range(4, vocab_size):  # skip special tokens\n",
    "    word = reverse_word_index.get(i, None)\n",
    "    if word:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fc7cd98-06ca-43a0-8136-e75c5e451f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          1000000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1117377 (4.26 MB)\n",
      "Trainable params: 117377 (458.50 KB)\n",
      "Non-trainable params: 1000000 (3.81 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 12:25:21.901549: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=500,\n",
    "                    trainable=False))  # Freeze embeddings\n",
    "\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "506d71b1-08cd-4359-bf29-3e74d3cc2c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Pad sequences\n",
    "x_train = pad_sequences(x_train, maxlen=500, padding='post')\n",
    "x_test = pad_sequences(x_test, maxlen=500, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9ce9e-5094-4a33-9196-8836de48aa6e",
   "metadata": {},
   "source": [
    "Each review have different length. So we are going to use `pad_sequences()` function to ensure that all sequences are of the same length (`max_length=200`) by adding zeros at the end (`padding='post'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b96d962-ce50-44c3-81b3-8cfc7049ee6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "157/157 [==============================] - 79s 493ms/step - loss: 0.6914 - accuracy: 0.5051 - val_loss: 0.6879 - val_accuracy: 0.5186\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 83s 528ms/step - loss: 0.6827 - accuracy: 0.5379 - val_loss: 0.6917 - val_accuracy: 0.5096\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 85s 543ms/step - loss: 0.6888 - accuracy: 0.5148 - val_loss: 0.6871 - val_accuracy: 0.5188\n",
      "Epoch 4/5\n",
      "157/157 [==============================] - 87s 551ms/step - loss: 0.6866 - accuracy: 0.5124 - val_loss: 0.6895 - val_accuracy: 0.5190\n",
      "Epoch 5/5\n",
      "157/157 [==============================] - 87s 557ms/step - loss: 0.6862 - accuracy: 0.5239 - val_loss: 0.6830 - val_accuracy: 0.5254\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f02ffa3-677b-4066-96e9-ebfc142f7519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 41s 53ms/step - loss: 0.6855 - accuracy: 0.5186\n",
      "Test Accuracy: 0.5186\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db03ad-a175-42e1-8d85-08d64fd582f8",
   "metadata": {},
   "source": [
    "First we are getting `word and index` by using function `get_word_index()`. Those are the top 10000 words from imbd reviews. So we can use each word for getting that perticular words vectors from glove embedding and store it as key and vector as value in `embedding_matrix`. Because we are going to pass that embedding_matrix dictionary to the `Embedding()` layer.\n",
    "\n",
    "\n",
    "#### Why do we pass embedding_matrix to Embedding()?\n",
    "The `Embedding()` layer in Keras is responsible for converting each word (represented by an integer index) into a dense vector of fixed size (like 100 dimensions). But instead of learning these word vectors from scratch, we can use pre-trained word vectors like GloVe `(embedding_matrix in our case)` â€” which are already trained on a huge dataset and capture word meanings well.\n",
    "\n",
    "\n",
    "When you pass `weights=[embedding_matrix]` into the Embedding layer, you're telling the model:\n",
    "\"Use these pre-trained GloVe vectors instead of learning them from scratch.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96df8acf-c2ea-454f-a28d-620084b3865f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
